{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blogpost: training and serving Tensorflow models with tf.Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is a high-level interface for neural networks that runs on top of multiple backends. Its functional API is very user-friendly, yet flexible enough to build all kinds of applications. Keras quickly gained traction after its introduction and in 2017, the Keras API was integrated into core Tensorflow as ```tf.keras```. Although ```tf.keras``` and Keras have separate code bases, they are tightly coupled and with the [updated documentation](https://www.tensorflow.org/tutorials/) and [programmer guides](https://www.tensorflow.org/guide/keras) as of Tensorflow 1.9, ```tf.keras``` is clearly the high level API to look for when building neural networks with Tensorflow.\n",
    "\n",
    "In this notebook, we will work through the process of training, exporting and serving a neural network with ```tf.keras```. As an example, we will train a convolutional neural network on the Kaggle Planet dataset to predict labels for satellite images of the Amazon forest. The goal is to illustrate an end-to-end pipeline for a real-world use case. **Note that you'll need to install the nightly version of TensorFlow, 1.11.0, to follow along until the end.**\n",
    "\n",
    "Also, when running on Google colab, make sure to change the runtime settings to Python 3. People who want to run the notebook on their own machine can use the requirements.txt file to set up their virtual environment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is available for download on [Kaggle](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/data). The code below will download the required data automatically. You'll need to create a [Kaggle API token](https://github.com/Kaggle/kaggle-api#api-credentials) first and upload it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the latest version of TF is installed\n",
    "!pip install tf-nightly\n",
    "!pip install h5py # required for saving Keras models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Upload the API token.\n",
    "def get_kaggle_credentials():\n",
    "    token_dir = os.path.join(os.path.expanduser(\"~\"),\".kaggle\")\n",
    "    token_file = os.path.join(token_dir, \"kaggle.json\")\n",
    "    if not os.path.isdir(token_dir):\n",
    "        os.mkdir(token_dir)\n",
    "    try:\n",
    "        with open(token_file,'r') as f:\n",
    "            pass\n",
    "    except IOError as no_file:\n",
    "        try:\n",
    "            from google.colab import files\n",
    "        except ImportError:\n",
    "            raise no_file\n",
    "\n",
    "        uploaded = files.upload()\n",
    "\n",
    "        if \"kaggle.json\" not in uploaded:\n",
    "            raise ValueError(\"You need an API key! see: \"\n",
    "                           \"https://github.com/Kaggle/kaggle-api#api-credentials\")\n",
    "        with open(token_file, \"wb\") as f:\n",
    "            f.write(uploaded[\"kaggle.json\"])\n",
    "        os.chmod(token_file, 600)\n",
    "\n",
    "get_kaggle_credentials()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the data. In this notebook, we will only require the training data from the competition. Make sure the accept the [competition rules](https://www.kaggle.com/c/planet-understanding-the-amazon-from-space/rules) first so that you can actually download the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kaggle\n",
    "# This will download the data (600 MB)\n",
    "competition_name = 'planet-understanding-the-amazon-from-space'\n",
    "\n",
    "kaggle.api.competition_download_file(competition_name, file_name='train-jpg.tar.7z')\n",
    "\n",
    "# This will extract the data\n",
    "import subprocess\n",
    "subprocess.call('7z x ./train-jpg.tar.7z'.split(' '))\n",
    "\n",
    "import tarfile\n",
    "with tarfile.open('./train-jpg.tar', 'r:') as f:\n",
    "    f.extractall()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data consists of approximately 40000 labeled images of the Amazon rain forest. Each image is associated with: \n",
    "    * Exactly one out of four possible 'weather' labels: clear, haze, cloudy or partly cloudy\n",
    "    * One or more out of 13 possible 'ground' labels: agriculture, bare_ground, habitation, road, water...  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I constructed a Pandas Dataframe with columns for the image names and the weather and ground labels encoded as one-hot binary vectors. The dataframe is available as a .csv file on my github page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv('https://raw.githubusercontent.com/sdcubber/keras-training-serving/master/KagglePlanetMCML.csv')\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to train a model that can accurately predict these labels for new images. We'll try to do this with a network that has two separate outputs for the weather and the ground labels. Predicting the weather labels is an example of a *multi-class classification* problem, whereas the ground labels can be modeled as a *multi-label classification* problem. Therefore, the loss function for both outputs will be different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specifying the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a computer vision problem, we will use a convolutional neural network. We will build our own model from scratch. We will go for a fairly classical configuration with some convolutional layers, relu activations and two dense classifiers on top. If you are running this notebook without a GPU, it's best to pick a small image size to reduce the training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "IM_SIZE = 16 # image size\n",
    "\n",
    "image_input = tf.keras.Input(shape=(IM_SIZE, IM_SIZE, 3), name='input_layer')\n",
    "\n",
    "# Some convolutional layers\n",
    "conv_1 = tf.keras.layers.Conv2D(32,\n",
    "                                kernel_size=(3, 3),\n",
    "                                padding='same',\n",
    "                                activation='relu')(image_input)\n",
    "conv_1 = tf.keras.layers.MaxPooling2D(padding='same')(conv_1)\n",
    "conv_2 = tf.keras.layers.Conv2D(32,\n",
    "                                kernel_size=(3, 3),\n",
    "                                padding='same',\n",
    "                                activation='relu')(conv_1)\n",
    "conv_2 = tf.keras.layers.MaxPooling2D(padding='same')(conv_2)\n",
    "\n",
    "# Flatten the output of the convolutional layers\n",
    "conv_flat = tf.keras.layers.Flatten()(conv_2)\n",
    "\n",
    "# Some dense layers with two separate outputs\n",
    "fc_1 = tf.keras.layers.Dense(128,\n",
    "                             activation='relu')(conv_flat)\n",
    "fc_1 = tf.keras.layers.Dropout(0.2)(fc_1)\n",
    "fc_2 = tf.keras.layers.Dense(128,\n",
    "                             activation='relu')(fc_1)\n",
    "fc_2 = tf.keras.layers.Dropout(0.2)(fc_2)\n",
    "\n",
    "# Output layers: separate outputs for the weather and the ground labels\n",
    "weather_output = tf.keras.layers.Dense(4,\n",
    "                                       activation='softmax',\n",
    "                                       name='weather')(fc_2)\n",
    "ground_output = tf.keras.layers.Dense(13,\n",
    "                                      activation='sigmoid',\n",
    "                                      name='ground')(fc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two output layers, so these should be passed as a list of outputs when specifying the model. Note the different activation functions for the weather and the ground output layers. Conveniently, the ```tf.keras``` implementation of ```Model``` comes with the handy ```summary()``` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=image_input, outputs=[weather_output, ground_output])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon compiling the model, the two different loss functions can be provided as a dictionary that maps tensor names to losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss={'weather': 'categorical_crossentropy',\n",
    "                    'ground': 'binary_crossentropy'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling a model initializes it with random weights and also allows us to choose an optimization algorithm for training the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the model! I will be training this model on my laptop, which does not have enough RAM to take the entire dataset into memory. With image data, this is very often the case. Keras provides the ```model.fit_generator()``` method that can use a custom Python generator yielding images from disc for training. However, as of Keras 2.0.6, we can use the ```Sequence``` object instead of a generator which allows for safe multiprocessing which means significant speedups and less risk of bottlenecking your GPU if you have one. The Keras documentation already provides good example code, which I will customize a bit to:\n",
    "* make it work with a dataframe that maps image names to labels\n",
    "* shuffle the training data after every epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from tensorflow.python.keras.preprocessing.image import img_to_array as img_to_array\n",
    "from tensorflow.python.keras.preprocessing.image import load_img as load_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, size):\n",
    "    return img_to_array(load_img(image_path, target_size=(size, size))) / 255.\n",
    "\n",
    "class KagglePlanetSequence(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    Custom Sequence object to train a model on out-of-memory datasets. \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, data_path, im_size, batch_size, mode='train'):\n",
    "        \"\"\"\n",
    "        df: pandas dataframe that contains columns with image names and labels\n",
    "        data_path: path that contains the training images\n",
    "        im_size: image size\n",
    "        mode: when in training mode, data will be shuffled between epochs\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.batch_size = batch_size\n",
    "        self.im_size = im_size\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Take labels and a list of image locations in memory\n",
    "        self.wlabels = self.df['weather_labels'].apply(lambda x: ast.literal_eval(x)).tolist()\n",
    "        self.glabels = self.df['ground_labels'].apply(lambda x: ast.literal_eval(x)).tolist()\n",
    "        self.image_list = self.df['image_name'].apply(lambda x: os.path.join(data_path, x + '.jpg')).tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(math.ceil(len(self.df) / float(self.batch_size)))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffles indexes after each epoch\n",
    "        self.indexes = range(len(self.image_list))\n",
    "        if self.mode == 'train':\n",
    "            self.indexes = random.sample(self.indexes, k=len(self.indexes))\n",
    "\n",
    "    def get_batch_labels(self, idx): \n",
    "        # Fetch a batch of labels\n",
    "        return [self.wlabels[idx * self.batch_size: (idx + 1) * self.batch_size],\n",
    "                self.glabels[idx * self.batch_size: (idx + 1) * self.batch_size]]\n",
    "\n",
    "    def get_batch_features(self, idx):\n",
    "        # Fetch a batch of images\n",
    "        batch_images = self.image_list[idx * self.batch_size: (1 + idx) * self.batch_size]\n",
    "        return np.array([load_image(im, self.im_size) for im in batch_images])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.get_batch_features(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "seq = KagglePlanetSequence(df_train,\n",
    "                       './train-jpg/',\n",
    "                       im_size=IM_SIZE,\n",
    "                       batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ```Sequence``` object can be used instead of a custom generator together with ```fit_generator()``` to train the model. Note that there is no need to provide the number of steps per epoch, since the ```__len__``` method implements that logic for the generator. Furthermore, ```tf.keras``` provides access to all the available Keras callbacks that can be used to enhance the training loop. These can be quite powerful and provide options for early stopping, learning rate scheduling, storing files for TensorBoard... Here, we will use the ```ModelCheckPoint``` callback to save the model after every epoch so that we can pick up training afterwards if we want. By default, the model architecture, training configuration, state of the optimizer and the weights are stored, such that the entire model can be recreated from a single file.\n",
    "Let's train the model for a single epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('./model.h5', verbose=1)\n",
    "]\n",
    "\n",
    "model.fit_generator(generator=seq,\n",
    "                    verbose=1, \n",
    "                    epochs=1,\n",
    "                    use_multiprocessing=False,\n",
    "                    workers=1,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose that we want to finetune the model in a later stage, we can simply read the model file and pick up training even without explicitly recompiling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_model = tf.keras.models.load_model('./model.h5')\n",
    "another_model.fit_generator(generator=seq, verbose=1, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, it's good to verify that our ```Sequence``` effectively passes over all the data by instantiating a ```Sequence``` in test mode (that is, without shuffling) and using it to make predictions for the entire dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = KagglePlanetSequence(df_train,\n",
    "                       './train-jpg/',\n",
    "                       im_size=IM_SIZE,\n",
    "                       batch_size=32,\n",
    "                       mode='test') # test mode disables shuffling\n",
    "\n",
    "predictions = model.predict_generator(generator=test_seq, verbose=1)\n",
    "# We get a list of two prediction arrays, for weather and for label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(predictions[1])  == len(df_train) # Total number of images in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wait, what about the Dataset API?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```tf.data``` API is a powerful library that allows to consume data from various sources and pass it to TensorFlow models. Can we train our ```tf.keras``` model using the ```tf.data``` API instead of with the ```Sequence``` object? Yes. First of all, let's serialize the images and labels together into a ```TFRecordfile```, which is the recommended format for serializing data in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize images, together with labels, to TF records\n",
    "def _bytes_feature(value):\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "tf_records_filename = './/KagglePlanetTFRecord_{}'.format(IM_SIZE)\n",
    "writer = tf.python_io.TFRecordWriter(tf_records_filename)\n",
    "\n",
    "# List of image paths, np array of labels\n",
    "im_list = [os.path.join('./train-jpg/', v + '.jpg') for v in df_train['image_name'].tolist()]\n",
    "w_labels_arr = np.array([ast.literal_eval(l) for l in df_train['weather_labels']])\n",
    "g_labels_arr = np.array([ast.literal_eval(l) for l in df_train['ground_labels']])\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    w_labels = w_labels_arr[i].astype(np.float32)\n",
    "    g_labels = g_labels_arr[i].astype(np.float32)\n",
    "    im = np.array(img_to_array(load_img(im_list[i], target_size=(IM_SIZE, IM_SIZE))) / 255.)\n",
    "    w_raw = w_labels.tostring()\n",
    "    g_raw = g_labels.tostring()\n",
    "    im_raw = im.tostring()\n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={'image': _bytes_feature(im_raw),\n",
    "                                                                  'weather_labels': _bytes_feature(w_raw),\n",
    "                                                                  'ground_labels': _bytes_feature(g_raw)}))\n",
    "    \n",
    "    writer.write(example.SerializeToString())\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After dumping the images and the labels into a ```TFRecordfile```, we can come up with another generator using the ```tf.data``` API. The idea is to instantiate a ```TFRecordDataset``` from our file and tell it how to parse the serialized data using the ```map()``` operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import FixedLenFeature\n",
    "featdef = {\n",
    "           'image': FixedLenFeature(shape=[], dtype=tf.string),\n",
    "           'weather_labels': FixedLenFeature(shape=[], dtype=tf.string),\n",
    "           'ground_labels': FixedLenFeature(shape=[], dtype=tf.string)\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_record(example_proto, clip=False):\n",
    "    ex = tf.parse_single_example(example_proto, featdef)\n",
    "    \n",
    "    im = tf.decode_raw(ex['image'], tf.float32)\n",
    "    im = tf.reshape(im, (IM_SIZE, IM_SIZE, 3))\n",
    "    \n",
    "    weather = tf.decode_raw(ex['weather_labels'], tf.float32)\n",
    "    ground = tf.decode_raw(ex['ground_labels'], tf.float32)\n",
    "    \n",
    "    return im, (weather, ground)\n",
    "\n",
    "# Construct a dataset iterator\n",
    "batch_size = 32\n",
    "ds_train = tf.data.TFRecordDataset('./KagglePlanetTFRecord_{}'.format(IM_SIZE)).map(_parse_record)\n",
    "ds_train = ds_train.repeat().shuffle(1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```Dataset``` objects provide multiple methods to produce iterator objects to loop over the data. However, as of TensorFlow 1.9, we can simply pass our ```ds_train``` directly to ```model.fit()``` to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Model(inputs=image_input, outputs=[weather_output, ground_output])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss={'weather': 'categorical_crossentropy',\n",
    "                    'ground': 'binary_crossentropy'})\n",
    "\n",
    "history = model.fit(ds_train, \n",
    "                    steps_per_epoch=100, # let's just take some steps\n",
    "                    epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works nicely. This way of working opens up ```tf.keras``` for people who are used to working with ```TFRecords```. If you want use validation data, you can just instantiate another ```Dataset``` with validation data and pass that as well to ```model.fit()```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serving the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we want to export our model in a format that the server can handle. TensorFlow provides the ```SavedModel``` format as a universal format for exporting models. Under the hood, our ```tf.keras``` model is fully specified in terms of TensorFlow objects, so we can export it just fine using Tensorflow methods.\n",
    "\n",
    "The main idea behind exporting a model is to specify an inference computation via a signature definition. A ```SignatureDef``` is fully specified in terms of input and output tensors and is eventually stored together with the model weights. However, TensorFlow provides a convenience function ```tf.saved_model.simple_save()``` which abstracts away some of these details and works fine for most use cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The easy way, with simple_save(), where the signature def is defined implicitly and stored with the\n",
    "# default graph way\n",
    "# The export path contains the name and the version of the model\n",
    "import shutil \n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.keras.backend.set_learning_phase(0)\n",
    "model = tf.keras.models.load_model('./model.h5')\n",
    "\n",
    "if os.path.exists('./PlanetModel/1'):\n",
    "    shutil.rmtree('./PlanetModel/1')\n",
    "    \n",
    "export_path = './PlanetModel/1'\n",
    "\n",
    "# Fetch the Keras session and save the model\n",
    "with tf.keras.backend.get_session() as sess:\n",
    "    tf.saved_model.simple_save(\n",
    "        sess,\n",
    "        export_path,\n",
    "        inputs={'input_image': model.input},\n",
    "        outputs={t.name:t for t in model.outputs})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These files can be used to the model as described in the blogpost by means of the [TensorFlow Serving library](https://www.tensorflow.org/serving/).  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 (tf-nightly-pip)",
   "language": "python",
   "name": "tf-nightly-pip"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
